{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhdzLxxJxUj628HYJZbcxr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MManuelG/trading_testing/blob/main/ticker_extractor_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PK5PW3BjG9p4",
        "outputId": "0c06dcc8-5b1a-4814-f484-a7c9c05cc728"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Tickers scraped: 101 (via len(tickerlist): 101)\n",
            "Valid Tickers scraped: 101 (via len(tickerlist): 101)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "FILENAME: ticker_extractor.py\n",
        "AUTHOR: Manu M\n",
        "CREATED: 2025-07-04\n",
        "UPDATED: -\n",
        "DESCRIPTION: script for extracting tickers from the wikipedia article of S&P100 using beautifulsoup webscraping\n",
        "\"\"\"\n",
        "\n",
        "############# IMPORT\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import yfinance as yf\n",
        "\n",
        "############# S&P 100 Wikipedia-Site\n",
        "url = \"https://en.wikipedia.org/wiki/S%26P_100\"\n",
        "\n",
        "############# Get & parse HTML\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "############# Find Table with tickers\n",
        "table = soup.find(\"table\", {\"class\": \"wikitable sortable\"})\n",
        "\n",
        "############# extract\n",
        "tickers = []\n",
        "counter=0\n",
        "for row in table.find_all(\"tr\")[1:]:\n",
        "    cols = row.find_all(\"td\")\n",
        "    if cols:\n",
        "        ticker = cols[0].text.strip()\n",
        "        # \"-\" instead of \".\" => \"BRK.B\" on yf is \"BRK-B\"\n",
        "        ticker = ticker.replace(\".\", \"-\")\n",
        "        tickers.append(ticker)\n",
        "        #print(ticker)\n",
        "        counter += 1\n",
        "\n",
        "\n",
        "############# validate tickers!\n",
        "valid_counter=0\n",
        "valid_tickers = []\n",
        "for ticker in tickers:\n",
        "    try:\n",
        "        data = yf.Ticker(ticker).info\n",
        "        if 'shortName' in data:  # Wenn Ticker g√ºltig\n",
        "            valid_tickers.append(ticker)\n",
        "            valid_counter += 1\n",
        "\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "#print(valid_tickers)\n",
        "print(f\"Total Tickers scraped: {counter} (via len(tickerlist): {len(tickers)})\")\n",
        "print(f\"Valid Tickers scraped: {valid_counter} (via len(tickerlist): {len(valid_tickers)})\")\n",
        "\n",
        "# if there are any invalid tickers, they will be added to this list\n",
        "invalid_tickers = list(set(tickers) - set(valid_tickers))\n",
        "\n",
        "if invalid_tickers: # print only if there are any invalid ones!\n",
        "  print(\"Invalid tickers:\", invalid_tickers)\n",
        "\n",
        "# write into .txt file called \"tickers.txt\"\n",
        "with open(\"tickers.txt\", \"w\") as f:\n",
        "    for ticker in tickers:\n",
        "        f.write(ticker + \"\\n\")"
      ]
    }
  ]
}